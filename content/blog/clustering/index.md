---
title: 聚类分析
summary: 几种聚类分析算法的原理和应用
date: 2025-12-04
authors:
  - admin
tags:
  - 机器学习
  - 无监督学习
---
## K-Means

K-Means是数据挖掘中一种**无监督学习**的聚类算法，它尝试将数据集划分成 K 个不同的、不重叠的子集（簇），使得每个簇内部的数据点尽可能相似，而簇与簇之间的数据点尽可能不相似。

### 核心思想

K-Means 的核心思想是**最小化簇内平方和（WCSS）**，即最小化每个数据点到其所属簇中心的距离平方和。




**目标：**

把数据分成 $K$ 个簇，让**同一簇里的点尽量彼此接近、不同簇之间尽量远**。
数学上是让簇内平方误差（SSE）最小：
$$

\text{SSE} = \sum_{k=1}^{K} \sum_{x_i \in C_k} | x_i - \mu_k |^2

$$
* $C_k$：第 $k$ 个簇
* $\mu_k$：第 $k$ 个簇的“中心”（均值）

---

**K-means 具体流程（算法步骤）**

给定：

* 数据点 ${x_1, x_2, \dots, x_n}$
* 想要的簇数 $K$

步骤：

1. **初始化中心（Centroids）**

   * 随机挑 $K$ 个样本点作为初始中心，或者用 K-means++ 等更聪明的方法。
   * 记为 $\mu_1, \mu_2, \dots, \mu_K$。

2. **E 步（分配步骤：Assignment）**

   * 对每个点 $x_i$，算它到各个中心的距离（一般是欧氏距离）
   * 把 $x_i$ 分配给最近的那个中心所属的簇
   * 得到 $K$ 个簇 $C_1,\dots,C_K$

3. **M 步（更新步骤：Update）**

   * 对每个簇 $C_k$，重新计算这个簇的中心：
    $$\mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i$$

4. **重复 2 和 3**

   * 直到：

     * 中心位置几乎不再变化，或者
     * 簇的分配不再变化，或者
     * 达到最大迭代次数。

5. 输出每个点的簇标签 + 每个簇中心 $\mu_k$。

注意点：

* 必须事先选好 $K$。
* 对初始中心比较敏感，容易落在“局部最优”。
* 特征量纲差异大时，要**标准化/归一化**（比如 z-score 或 min-max）。

---

### 例子

假设有 4 个点，只考虑 $(x,y)$ 两个特征：

* $A(1,1)$
* $B(1,2)$
* $C(4,4)$
* $D(5,5)$

想要分成 $K=2$ 个簇。

**1. 初始化中心**

随便选两个点当初始中心，例如：

* 簇1 中心 $M_1^{(0)} = A(1,1)$
* 簇2 中心 $M_2^{(0)} = D(5,5)$

**2. 第一次分配**

用欧氏距离的平方（省得写根号）：

* 点 A(1,1)

  * 到 $M_1(1,1)$：$(1-1)^2 + (1-1)^2 = 0$
  * 到 $M_2(5,5)$：$(1-5)^2 + (1-5)^2 = 4^2 + 4^2 = 16+16=32$
    ⇒ A 分到簇1

* 点 B(1,2)

  * 到 $M_1(1,1)$：$(1-1)^2 + (2-1)^2 = 0+1=1$
  * 到 $M_2(5,5)$：$(1-5)^2 + (2-5)^2 = 4^2 + 3^2 = 16+9=25$
    ⇒ B 分到簇1

* 点 C(4,4)

  * 到 $M_1(1,1)$：$(4-1)^2 + (4-1)^2 = 3^2 + 3^2 = 9+9=18$
  * 到 $M_2(5,5)$：$(4-5)^2 + (4-5)^2 = 1^2 + 1^2 = 1+1=2$
    ⇒ C 分到簇2

* 点 D(5,5)

  * 到 $M_1(1,1)$：$(5-1)^2 + (5-1)^2 = 4^2 + 4^2 = 16+16=32$
  * 到 $M_2(5,5)$：0
    ⇒ D 分到簇2

得到的簇：

* 簇1：{A(1,1), B(1,2)}
* 簇2：{C(4,4), D(5,5)}

**3. 第一次更新中心**

* 簇1 新中心：
$
  x: (1+1)/2 = 1,\quad y: (1+2)/2 = 1.5
  \Rightarrow M_1^{(1)} = (1, 1.5)
$

* 簇2 新中心：
$
  x: (4+5)/2 = 4.5,\quad y: (4+5)/2 = 4.5
  \Rightarrow M_2^{(1)} = (4.5, 4.5)
$

**4. 第二次分配（检查是否还会变）**

再算一次（同样用距离平方）：

* A(1,1)

  * 到 $M_1(1,1.5)$：$(1-1)^2 + (1-1.5)^2 = 0 + (-0.5)^2 = 0.25$
  * 到 $M_2(4.5,4.5)$：$(1-4.5)^2 + (1-4.5)^2 = (-3.5)^2 + (-3.5)^2 = 12.25+12.25=24.5$
    ⇒ 仍然簇1

* B(1,2)

  * 到 $M_1(1,1.5)$：$(1-1)^2 + (2-1.5)^2 = 0 + 0.5^2 = 0.25$
  * 到 $M_2(4.5,4.5)$：$(1-4.5)^2 + (2-4.5)^2 = (-3.5)^2 + (-2.5)^2 = 12.25+6.25=18.5$
    ⇒ 仍然簇1

* C(4,4)

  * 到 $M_1(1,1.5)$：$(4-1)^2 + (4-1.5)^2 = 3^2 + 2.5^2 = 9+6.25=15.25$
  * 到 $M_2(4.5,4.5)$：$(4-4.5)^2 + (4-4.5)^2 = (-0.5)^2 + (-0.5)^2 = 0.25+0.25 = 0.5$
    ⇒ 仍然簇2

* D(5,5)

  * 到 $M_1(1,1.5)$：$(5-1)^2 + (5-1.5)^2 = 4^2 + 3.5^2 = 16+12.25=28.25$
  * 到 $M_2(4.5,4.5)$：$(5-4.5)^2 + (5-4.5)^2 = 0.5^2 + 0.5^2 = 0.25+0.25=0.5$
    ⇒ 仍然簇2

簇分配没有变化，中心再算一遍也一样，所以算法**收敛**。

最终结果：

* 簇1：两个比较“左下”的点 A、B
* 簇2：两个比较“右上”的点 C、D




### 评估K-Means的效果

**1. 簇内平方和 (WCSS)**

WCSS 是最基本的度量，值越小表示聚类效果越好（簇内的数据点更紧凑）。

$$
\text{WCSS} = \sum_{j=1}^{K} \sum_{i \in \text{Cluster}_j} \| x_i - \mu_j \|^2
$$

**2. K 值的选择：肘部法则 (Elbow Method)**

* **方法：** 尝试不同的 $K$ 值，计算对应的 WCSS。
* **判断：** 绘制 WCSS 与 $K$ 值的关系图，曲线中 WCSS 显著下降速度变慢的点，形似“肘部”，这个点对应的 $K$ 值通常被认为是最佳的。

<figure style="text-align: center;">
  <img src="clustering1.png" alt="肘部法则" width="100%">
  <figcaption>图 1：肘部法则</figcaption>
</figure>

**3. 轮廓系数 (Silhouette Score)**


对某个样本点 (i)，定义两个量：

1. **簇内平均距离 (a(i))**

   * (i) 所在簇记为 (C)
   * (a(i)) = 点 (i) 到同簇里**其他所有点**的平均距离
   * 如果该簇只有一个点，可约定 (a(i)=0)

2. **最近“其他簇”的平均距离 (b(i))**

   * 对每一个**不包含 (i)** 的簇 (C')，计算：
     $$
     d(i, C') = \text{点 } i \text{ 到 } C' \text{ 中所有点的平均距离}
     $$
   * 然后取这些平均距离中的最小值：
     $$
     b(i) = \min_{C' \neq C} d(i, C')
     $$

  有了 (a(i))、(b(i))，轮廓系数定义为：

$$
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
$$


* **作用：** 结合了簇内聚性和簇间分离性的度量，得分范围在 $[-1, 1]$ 之间。
* **解释：**
    * **接近 1：** 样本与自己的簇高度相似，且与相邻簇差异大。
    * **接近 0：** 样本位于两个簇的边界上。
    * **接近 -1：** 样本可能被错误地分配给了错误的簇。

## License

Copyright 2025-present [Ginyee-W](https://ginyee-w.github.io/).
